文本和图像特征表示模块详解-多模态讲解系列(2)

爱奇艺短视频分类技术解析：https://www.infoq.cn/article/f49e-Gb1xQxh8DttFDgb


上一个文章讲的是层次分类体系的必要性，谈了一下为什么需要一级分类标签以及为何不直接一步到位分类到三级标签。

特征表示模块

文本表示：
对一个视频，我们能够想到的文本一般是：标题+简介+字幕。

那么我么如何对文本进行建模呢？我之前在github上更新了一个句向量模型综述，还没搬到公众号，大家可以去看一下。这里我简单说一下。

1. 词袋模型（基于统计和基于词向量），这种建模问题在于忽略了词序信息，可以使用n-gram进行缓解。
2. 基于任务（CNN/RNN），存在的问题是迁移性较差。如果是分类网络训练出来的CNN表达的句子向量迁移到情感分类效果不会很好。然后我们分开来说，CNN存在一个问题就是对长距离处理的不是很好。
因为它的本质是重视的n-gram内的语序信息。RNN存在的问题是训练速度慢，这没什么可说的，不能并行是硬伤。
还有其他建模方式就不多说了。我们来看爱奇艺的处理方式，一句话简单描述是“采用的是 BOW 和 CNN+Attention 方式完成文本表示的建模”

Bow使用一些人工特征加n-gram缓解自带的问题。CNN使用两个优化，提取信息使用一定步长的pooling，然后基于这个带有文本信息的表达做self-attention。


图像表示：
对于短视频来说的图像表示是什么？是封面。封面一般是从短视频精选出来的一帧，一定程度可以对文本信息进行补充。

对图像进行特征的抽取一般是有三种方式：

直接抽取特征
实现方式：把 ImageNet 预训练的模型作为特征抽取器，将模型的某一层或者某几层特征作为类型标签模型特征提取源。
缺点是效果比较差。
一般对应到NLP，大家可以想一下我们直接用Bert抽取出来的词向量做文本分类，效果也比较差。


finetune+抽取特征
把 ImageNet 预训练的模型以类型标签为目标进行 FineTune，然后将模型的某一层或者某几层特征作为类型标签模型特征提取源（因训练目标一致，一般选择最后一层即可达到较好的效果）。

大家仔细琢磨一下这个过程。如果我先在要做一个文本分类的任务，想使用LR做一个baseline。那么我的输入可以是这样的，使用bert对我要使用分类数据（注意是和LR一样的训练数据）进行FineTune
，然后使用这个模型做特征的抽取。

FineTune的任务和我LR要做的是一样的，那么bert抽取的特具有充足的意义表达，能够很好的迁移过来。

基于此，大家可以想一下，如果我使用bert做了文本情感分析的FineTune，然后抽取的特征做文本分类，效果会好吗？想一下。

这还有一个问题，从bert抽取出来的特征，我们需不需要随着模型进行修改？！！！！！！！

第三种方式：
把 ImageNet 预训练的模型嵌入到类型标签的模型当中，让图像的表示和其他特征的表示同时进行训练。

其实这种方式缺点很明显，耗时太大了，有种尾大不掉的感觉。

爱奇艺选择的第二种，模型选择是 Xception

特征融合怎么做？
也就是文本图像的特征怎么联系在一起。

三种：直接concat，方式简单，可以做一个简单的基线。
CentralNet 
LMF
三个模型我就不具体介绍了。我还可以给大家一个思考，attention

