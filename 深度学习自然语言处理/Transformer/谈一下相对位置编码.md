谈一下相对位置编码RPR

经过线性变化之后，正余弦函数表示的相对位置信息消失，所以需要优化。

一般来讲，谈到优化，三种比较有名：RPR； Transformer-XL；complex embeddings；

我在这个文章简单讲一下RPR。

老样子，不涉及到公式推导，尽量把我的理解讲出来。

#### RPR思路

RPR思路很简单，原始正余弦函数，是在输入的的时候与词向量相加作为输入，在attention丢失相对位置信息.

改进的话就是不在输入的时候进行位置编码，而是在attention中显示把相对位置信息加入进去。

#### 如何理解相对位置

绝对位置编码是在每个位置都对应一个唯一的位置编码信息，RPR把这一部分去掉去学习一个相对位置编码。

首先我们需要知道相对位置是有方向的的。

举个例子：”我/爱/中国/共产党“

”我“对”爱“的相对位置就是 -1， ”中国“对”爱“的相对位置就是 1。

所以方向不同，对应两个不同的相对位置，在学习的时候，一个距离，也就需要学习两个相对位置编码。

#### RPR修改思想

作者认为在相对位置小于4的时候，attention对相对位置比较敏感，在大于4之后，相对位置不敏感。所以窗口设置为4。

需要注意的是，窗口设为4，代表的当前位置左边4个，右边也有4个，再加上自己，就是一共9个位置，也就是：

$[i-4,i-3,i-2,i-1,i,i+1,i+2,i+3,i+4]$

注解：有方向

当你的attention进行到哪个单词的时候，你的 $i$ 就对应的是哪个位置。

还是上面那句话举例子。

如果此时的输入是“我”，那么用到的相对位置编码就是$[i,i+1,i+2,i+3]$

如果此时输入的是“爱”，那么这个时候用到的相对位置编码就是$[i-1,i,i+1,i+2]$

了解了这个，我们再谈一下这个相对位置信息是怎么显示加入进去的。

这个显示的加入分为两个部分。

第一个部分是在计算$e_{ij}$的时候，涉及到RPR的一个表征:$a_{ij}^{K}$，表示对 Q/K/V三者中的K做了修改。

第二个部分就是在计算$z_{i}$的时候，涉及到另一个RPR的表征:$a_{ij}^{V}$，表示对Q/K/V三者中的V做了修改。

两个部分的修改都是使用加法。



关于RPR大概就讲这么多吧。其实思路还是比较简单的，总结来说，就是把相对位置信息在attention之中，显势的加入进去，而不是在输入的时候与词向量相加。

如果觉得对您有点帮助，点个赞再走吧。

参考资料

[dasou](https://github.com/DA-southampton/NLP_ability/blob/master/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/Transformer/%E5%8E%9F%E7%89%88Transformer%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E7%A9%B6%E7%AB%9F%E6%9C%89%E6%B2%A1%E6%9C%89%E5%8C%85%E5%90%AB%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%E4%BF%A1%E6%81%AF.md)