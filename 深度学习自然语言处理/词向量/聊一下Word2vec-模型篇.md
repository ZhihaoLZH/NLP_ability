本文大概需要阅读 5.25 分钟



大概用三篇文章好好谈一下Word2vec，这篇主要是关于 Word2vec 的两种模型。



  **先问大家个问题：一个词通过Word2vec训练之后，可以得到几个词向量？**



如果您有点懵，说明我写对了，您慢慢看下去，码字不易，**请多多点赞，让更多人看到**，谢谢。



**词向量一般被认为是一个词的特征向量**，也就是说可以代表一个词的含义。一个中文词，比如"中国"这个词，是很难被神经网络直接作为输入，我们需要将词向量化（或者说数字化），才能更好的喂进神经网络。



**这个过程很类似于计算机在处理我们输入的文字的时候，会转化为二进制进行处理。**



只不过这个二进制表达规则我们会自己人为规定，而词向量这个向量表达根据不同的方式会有着不同的结果。**想一下，两者是不是很类似**。



谈到向量表达单词，一般首先想到的都是One-Hot编码。但是它是有缺点的。我这里谈两个缺点。



首先是**维度灾难**：如果我们有1万个单词，那么你的One-hot去表示每个单词的的时候，会转变为一个1万维度的向量。



那么在计算的时候会带来巨大的不便，而且向量矩阵极其稀疏，占据了太多不必要的内存。当然对于维度灾难，我们一般可以使用PCA等降维手段来缓解。



其次是**语义表达不足**。这一点很简单，”娱乐“与”八卦“两个词，通过One-hot编码得到的向量，计算Consine得到相似度为0。



这显然是不合适的。**One-Hot编码表示出来的词向量是两两正交的**，从余弦相似度的角度来看，是互不相关的，所以 One-Hot 不能很好的表达词语的相似性。



这个时候，我们再来看 Word2vec。首先，要明确一点，**Word2vec 不是一个模型，而是一个工具**。这个点虽然无伤大雅，但是每每看到有的文章说它是个模型，就有点...



**Word2vec 从整体上理解，就是包含两个模型（CBOW and Skip-gram）和两个高效的优化训练方式（负采样和层序softmax）**



这个文章主要谈一下两种模型。



先假定我们的窗口大小为2，也就是说中心词前面有两个词，后面有两个词，比如"我/永远/爱/中国/共产党"，抽象出来就是：



W_{t-2}，W_{t-1}，W_{t}，W_{t+1}，W_{t+2}



对于Skip-gram，中文我们叫跳字模型，使用中心词预测背景词。也就是用"爱"去预测其他的四个词。



对于CBOW，中文我们叫连续词袋模型，使用背景词预测中心词，也就是用"我"，"永远"，"中国"，"共产党" 去预测"爱"。



CBOW的模型架构，从整体上，我们可以分为三层：输入层，投影层，和输出层。



对于输入层，对应的是窗口中的单词，也就是例子中"我"，"永远"，"中国"，"共产党" 四个词的词向量，在投影层，将四个词的词向量进行相加求平均，输出层在没有优化的前提下，维度为词表大小，随后做 Softmax即可。



Skip-gram模型架构很类似，只不过可以省去投影层，因为输入为中心词，是一个单词的词向量，无从谈起求和与平均了。



接下来，我们来详细谈一下Skip-gram。



对于一个神经网络模型，我们需要确定我们的优化目标或者说目标函数是什么？



对于Skip-gram，其实很容易理解，就是我要最大化在给出中心词的条件下背景词出现的概率，公式如下



 \prod_{t=1}^T \ \prod_{-m\leq j \leq m,j\neq 0}  P(w^{t+j}|w^{t})



**这个公式对应着两个连乘，第一个连乘是对应 T 个输入样本，也就是说我们文本中的每个单词都要做中心词。第二个连乘代表着给定一个中心词的情况下，窗口中的单词出现的概率，内含相互独立的假设**。



在这里，**有一个细节点想要提醒大家**，**在词汇表中的每个单词，都是对应两个词向量的**，**一个是在作为中心词的时候的中心词向量，一个是在作为背景词时候的背景词向量**。



优化目标函数的时候，为了减少复杂度（也就是忽略 T），我们可以使用随机梯度下降，针对每一个样本我们都更新一次参数。



通过公式推导（略），我们能够观察到一个特点，就是每次更新参数，都会涉及到词典中的全部词汇，这是因为我们在做 Softmax 的时候，是分母是针对所有词汇的操作。



这样的操作的复杂度是 O(|V|)，其中|V|就是我们词汇表的大小。CBOW 其实是很类似的情况，每次更新都会涉及到我们的全部词汇。



于是，我们就需要对此进行优化，使用了两种近似的训练方式来高效的训练Word2vec，降低训练的复杂度。



**下一个文章谈一下优化方式。**



**如果觉得写的还行，帮忙点个赞或者在看，让更多人关注到我吧，感谢。**