Word2vec 涉及到两种优化方式，一种是负采样，一种是层序Softmax



先谈一下负采样，以跳字模型为例。中心词生成背景词可以由两个相互独立事件的联合组成来近似（引自李沐大神的讲解）。



第一个事件是，中心词和背景词同时出现在窗口中。第二个事件是，中心词和K个噪声词不同时出现在窗口数据中，其中噪声词由噪声分布随机生成。



这里我们就可以知道上一个文章开头说到的，负采样是一种等价操作还是近似操作？我们在第二个事件中，使用了K个噪声词。但是实际上呢？应该远远大于K。



还是那个例子，句子为"我/永远/爱/中国/共产党"，中心词为'爱'，我们在选择噪声词的时候，选择了K个，但是实际上，在词汇表中，排除掉'我'，'永远'，'中国'，'共产党' 这四个词汇的其他词都可以算做我的噪声词，然而为了减少复杂度，我只选择了其中的K个，所以当然应该是近似了。



接下来，我们看层序Softmax。



层序Softmax 对应的就是在输出层使用一个霍夫曼树，代替了原本在输出层统一进行的softmax。


首先，我们需要了解霍夫曼树在这里是如何构建的。



简单讲，霍夫曼树是一个二叉树，以语料中出现过的词当做叶子节点，以各词在语料中出现的次数当做权值进行构造。其中叶子节点有N个，就是词典的大小，非叶子节点有N-1个（包括根节点）。



比如说我的所有文章中，“共产党”这个词出现了 100次，是最大的，那么根节点的左分支（或者右分支）就对应着”共产党“这个词，另一个分支做与根节点相同的操作，找到排除”共产党“这个词之外的所有词中最大的词，比如”中国“作为其中的左分支（或者右分支），以此类推，一个霍夫曼树就成功构建。



霍夫曼树中，我们需要注意的是，每个非叶子节点对应一个向量，每个叶子节点对应一个向量。两种向量都会随着模型的训练进行更新。



其中叶子节点的向量就是我们的词向量，而非叶子节点上的向量就是没有什么实际含义，它的作用就是帮助我们计算模型在霍夫曼树上不断的进行二分类时候的概率。



以上面那句话为例，我们现在中心词为‘爱’，然后，我要预测背景词‘中国’。首先我们要确定的是我的叶子节点是包含所有单词的，也就是包含了我这个简单句子的五个单词（不考虑前期数据清洗低频率词的情况）。



也就是说，在这个霍夫曼树上，有且仅有一条路径，让我从根节点出发，经过多次判断（也就是说走过了多个非叶子节点），最终走到了“中国”这个叶子节点，对应的概率就是每个节点概率的连乘。



然后这个时候，我们想一下霍夫曼树是不是一种近似？



当然，我们每更新一个词向量，只是涉及到了可以到达叶子节点的这一条路径上节点。所以复杂度就是树的高度，也就是 O(log|V|)