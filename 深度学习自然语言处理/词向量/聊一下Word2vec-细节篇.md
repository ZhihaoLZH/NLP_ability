**本文大概需要阅读 2.75 分钟**

**以Q&A的形式，聊一聊Word2vec的细节点**

**Word2vec训练参数的选定?**

首先根据具体任务，选一个领域相似的语料，在这个条件下，语料越大越好。然后下载一个 word2vec 的新版（14年9月更新），语料小（小于一亿词，约 500MB 的文本文件）的时候用 Skip-gram 模型，语料大的时候用 CBOW 模型。最后记得设置迭代次数为三五十次，维度至少选 50，就可以了。（引自 《How to Generate a Good Word Embedding》）

**Word2vec模型是如何获得词向量的?**

很多文章在解释的时候，会说对一个词通过One-hot编码，然后通过隐层训练，得到的矩阵就对应的词表词向量。这个我觉得是不准确的。

首先，在前面文章也说过了，Word2vec是不涉及到隐层的，CBOW有投影层，只是简单的求和平均，Skip-gram没有投影层，就是中心词接了一个霍夫曼树。

在此情况下，词向量是怎么来的？首先，要明确一点，以Skip-gram为例，我们的输入的中心词的词向量其实不是One-hot编码，而是随机初始化的一个词向量，它会随着模型训练而更新。

需要注意的一个细节点是，每个单词都会对应两个词向量，一个是作为中心词的时候的词向量，一个是作为背景词的时候的词向量。大家一般选择第一种。

**CBOW和skip-gram相较而言，彼此相对适合哪些场景**

CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。

为什么这么说？Skip-gram 是中心词预测背景词，假如我们有K个背景词，那么对于一个中心词来说，就是做了K次训练，那么词向量就回得到更加充分的训练。

而对于CBOW，我们是背景词预测中心词，相当于只是做了一次训练。想一下是不是这么回事？

简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。

**负采样的特点**

首先对基于负采样的技术，我们更新的权重只是采样集合，减少了训练量，同时效果上来说，中心词一般来说只和上下文有关，更新其他词的权重并不重要，所以在降低计算量的同时，效果并没有变差。

**负采样具体实施细节**

就是创建两个线段，第一个线段切开词表大小的份数，每个份数的长度和频率正比。

第二个线段均分M个，然后随机取整数，整数落在第二个线段哪里，然后取第一个线段对应的词，如果碰到是自己，那么就跳过。

在代码实现中，第一个线段的长度不仅仅是频率，而是一个3/4的幂次方，第二个线段切分为10的8次方个段数

**如果觉得对您有所帮助，帮忙点个在看或者赞，谢谢!**