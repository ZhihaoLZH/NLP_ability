CBOW和skip-gram相较而言，彼此相对适合哪些场景

先用一句话来个结论：CBOW比Skip-gram 训练速度快，但是Skip-gram可以得到更好的词向量表达。

为什么这么说？

因为我们知道两种优化方式只是对softmax的近似优化，不会影响最终结果，所以这里，我们讨论的时候，为了更加的清晰讲解，不考虑优化的情况。

使用一句话作为一个例子： “我/永远/爱/中国/共产党”

先说CBOW，我们想一下，它的情况是使用周围词预测中心词。如果“爱”是中心词，别的是背景词。对于“爱”这个中心词，只是被预测了一次。

对于Skip-gram，同样，我们的中心词是“爱”，背景词是其他词，对于每一个背景词，我们都需要进行一次预测，每进行一次预测，我们都会更新一次词向量。也就是说，相比CBOW，我们的词向量更新了2k次（假设K为窗口，那么窗口内包含中心词就有2k+1个单词）

想一下是不是这么回事？Skip-gram被训练的次数更多，那么词向量的表达就会越丰富。

如果语料库中，我们的的低频词很多，那么使用Skip-gram就会得到更好的低频词的词向量的表达，相应的训练时长就会更多。

简单来说，我们视线回到一个大小为K的训练窗口（窗口内全部单词为2k+1个），CBOW只是训练一次，Skip-gram 则是训练了2K次。当然是Skip-gram词向量会更加的准确一点，相应的会训练的慢一点。

欢迎大佬拍砖